#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ RAG –æ—Ç–≤–µ—Ç–æ–≤
–ú–µ—Ç—Ä–∏–∫–∏: line_numbers, file_path, code_snippet, keywords
"""
import json
import re
import sys
from pathlib import Path
import requests
from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QAQualityEvaluator:
    def __init__(self, api_url: str = "http://192.168.2.107:8003/api/query"):
        self.api_url = api_url
        self.results = []
        
    def evaluate_answer(self, question: str, answer: str, expected_keywords: List[str]) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        metrics = {}
        
        # 1. has_line_numbers: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç "—Å—Ç—Ä–æ–∫–∏ XX-YY" –∏–ª–∏ "üìç –°—Ç—Ä–æ–∫–∏: XX-YY" (–≤–µ—Å 40%)
        line_pattern = r'—Å—Ç—Ä–æ–∫[–∞–µ–∏]?:?\s+\d+-\d+|—Å—Ç—Ä–æ–∫[–∞–µ–∏]?\s+\d+|line[s]?\s+\d+-\d+|üìç\s*[–°—Å]—Ç—Ä–æ–∫[–∞–µ–∏]?:?\s+\d+-\d+'
        has_lines = bool(re.search(line_pattern, answer, re.IGNORECASE))
        metrics['has_line_numbers'] = 1.0 if has_lines else 0.0
        
        # 2. has_file_path: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (–≤–µ—Å 20%)
        file_pattern = r'[\w/]+\.py|[\w/]+\.md'
        has_file = bool(re.search(file_pattern, answer))
        metrics['has_file_path'] = 1.0 if has_file else 0.0
        
        # 3. has_code_snippet: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –±–ª–æ–∫ –∫–æ–¥–∞ (–≤–µ—Å 20%)
        code_pattern = r'```[\s\S]*?```'
        has_code = bool(re.search(code_pattern, answer))
        metrics['has_code_snippet'] = 1.0 if has_code else 0.0
        
        # 4. keyword_match: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (–≤–µ—Å 20%)
        if expected_keywords:
            answer_lower = answer.lower()
            matched_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]
            metrics['keyword_match'] = len(matched_keywords) / len(expected_keywords)
        else:
            metrics['keyword_match'] = 1.0
        
        # –û–±—â–∏–π score —Å –≤–µ—Å–∞–º–∏
        total_score = (
            metrics['has_line_numbers'] * 0.40 +
            metrics['has_file_path'] * 0.20 +
            metrics['has_code_snippet'] * 0.20 +
            metrics['keyword_match'] * 0.20
        )
        metrics['total_score'] = total_score
        
        return metrics
    
    def test_query(self, question: str, project: str = "staffprobot") -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ RAG"""
        try:
            response = requests.post(
                self.api_url,
                json={"query": question, "project": project},
                timeout=30
            )
            
            if response.status_code != 200:
                return {"error": f"HTTP {response.status_code}"}
            
            data = response.json()
            return {
                "answer": data.get("answer", ""),
                "sources": data.get("sources", []),
                "sources_count": len(data.get("sources", []))
            }
        
        except Exception as e:
            return {"error": str(e)}
    
    def run_evaluation(self, test_file: str = "/app/tests/test_staffprobot_queries.json"):
        """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ"""
        logger.info("üìä –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ RAG")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            test_queries = test_data.get("test_queries", [])
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ—Å—Ç–æ–≤: {e}")
            return
        
        logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_queries)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤\n")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        for i, query in enumerate(test_queries, 1):
            logger.info(f"[{i}/{len(test_queries)}] {query['category']} ({query['difficulty']})")
            logger.info(f"‚ùì {query['query']}")
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            result = self.test_query(query['query'])
            
            if "error" in result:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}\n")
                self.results.append({
                    "query": query,
                    "error": result["error"],
                    "metrics": None
                })
                continue
            
            # –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–∞
            metrics = self.evaluate_answer(
                query['query'],
                result['answer'],
                query.get('expected_keywords', [])
            )
            
            # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
            logger.info(f"üìù –û—Ç–≤–µ—Ç ({len(result['answer'])} —Å–∏–º–≤.)")
            logger.info(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {result['sources_count']}")
            logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏:")
            logger.info(f"   ‚Ä¢ Line numbers: {metrics['has_line_numbers']*100:.0f}%")
            logger.info(f"   ‚Ä¢ File path: {metrics['has_file_path']*100:.0f}%")
            logger.info(f"   ‚Ä¢ Code snippet: {metrics['has_code_snippet']*100:.0f}%")
            logger.info(f"   ‚Ä¢ Keywords: {metrics['keyword_match']*100:.0f}%")
            logger.info(f"   ‚≠ê TOTAL: {metrics['total_score']*100:.0f}%")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–≤—å—é –æ—Ç–≤–µ—Ç–∞
            preview = result['answer'][:150].replace('\n', ' ')
            logger.info(f"   üí¨ {preview}...\n")
            
            self.results.append({
                "query": query,
                "result": result,
                "metrics": metrics
            })
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._print_summary()
    
    def _print_summary(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        logger.info("\n" + "=" * 80)
        logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("=" * 80)
        
        successful = [r for r in self.results if r.get('metrics')]
        failed = [r for r in self.results if not r.get('metrics')]
        
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(successful)}/{len(self.results)}")
        if failed:
            logger.info(f"‚ùå –û—à–∏–±–æ–∫: {len(failed)}")
        
        if not successful:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_metrics = {
            'has_line_numbers': sum(r['metrics']['has_line_numbers'] for r in successful) / len(successful),
            'has_file_path': sum(r['metrics']['has_file_path'] for r in successful) / len(successful),
            'has_code_snippet': sum(r['metrics']['has_code_snippet'] for r in successful) / len(successful),
            'keyword_match': sum(r['metrics']['keyword_match'] for r in successful) / len(successful),
            'total_score': sum(r['metrics']['total_score'] for r in successful) / len(successful)
        }
        
        logger.info(f"\nüìà –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
        logger.info(f"   ‚Ä¢ Has Line Numbers: {avg_metrics['has_line_numbers']*100:.1f}% {'‚úÖ' if avg_metrics['has_line_numbers'] >= 0.85 else '‚ùå'} (—Ü–µ–ª—å: >85%)")
        logger.info(f"   ‚Ä¢ Has File Path: {avg_metrics['has_file_path']*100:.1f}% {'‚úÖ' if avg_metrics['has_file_path'] >= 0.95 else '‚ùå'} (—Ü–µ–ª—å: >95%)")
        logger.info(f"   ‚Ä¢ Has Code Snippet: {avg_metrics['has_code_snippet']*100:.1f}% {'‚úÖ' if avg_metrics['has_code_snippet'] >= 0.80 else '‚ùå'} (—Ü–µ–ª—å: >80%)")
        logger.info(f"   ‚Ä¢ Keyword Match: {avg_metrics['keyword_match']*100:.1f}% {'‚úÖ' if avg_metrics['keyword_match'] >= 0.90 else '‚ùå'} (—Ü–µ–ª—å: >90%)")
        logger.info(f"   ‚≠ê TOTAL SCORE: {avg_metrics['total_score']*100:.1f}%")
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã (score < 50%)
        problematic = [r for r in successful if r['metrics']['total_score'] < 0.5]
        if problematic:
            logger.info(f"\n‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã ({len(problematic)}):")
            for r in problematic[:5]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 5
                logger.info(f"   ‚Ä¢ [{r['metrics']['total_score']*100:.0f}%] {r['query']['query'][:60]}...")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        excellent = [r for r in successful if r['metrics']['total_score'] >= 0.85]
        good = [r for r in successful if 0.70 <= r['metrics']['total_score'] < 0.85]
        fair = [r for r in successful if 0.50 <= r['metrics']['total_score'] < 0.70]
        poor = [r for r in successful if r['metrics']['total_score'] < 0.50]
        
        logger.info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É:")
        logger.info(f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω–æ (>85%): {len(excellent)} ({len(excellent)/len(successful)*100:.1f}%)")
        logger.info(f"   ‚Ä¢ –•–æ—Ä–æ—à–æ (70-85%): {len(good)} ({len(good)/len(successful)*100:.1f}%)")
        logger.info(f"   ‚Ä¢ –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ (50-70%): {len(fair)} ({len(fair)/len(successful)*100:.1f}%)")
        logger.info(f"   ‚Ä¢ –ü–ª–æ—Ö–æ (<50%): {len(poor)} ({len(poor)/len(successful)*100:.1f}%)")
        
        logger.info("=" * 80)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._save_results()
    
    def _save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏"""
        output_file = "/tmp/qa_evaluation_results.json"
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        output_data = {
            "total_queries": len(self.results),
            "successful": len([r for r in self.results if r.get('metrics')]),
            "failed": len([r for r in self.results if not r.get('metrics')]),
            "results": []
        }
        
        for r in self.results:
            output_data["results"].append({
                "question": r['query']['query'],
                "category": r['query']['category'],
                "difficulty": r['query']['difficulty'],
                "metrics": r.get('metrics'),
                "answer_preview": r.get('result', {}).get('answer', '')[:200] if r.get('result') else None,
                "sources_count": r.get('result', {}).get('sources_count', 0) if r.get('result') else 0
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

def main():
    evaluator = QAQualityEvaluator()
    evaluator.run_evaluation()

if __name__ == "__main__":
    main()

