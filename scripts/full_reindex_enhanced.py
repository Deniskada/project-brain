#!/usr/bin/env python3
"""
ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑÐ¼Ð¸:
1. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ QA Ð¿Ð°Ñ€ Ð¸Ð· ÐºÐ¾Ð´Ð°
2. Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
3. Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹
4. ÐžÑ†ÐµÐ½ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
"""
import asyncio
import sys
import logging
from pathlib import Path

sys.path.insert(0, '/app')

from backend.indexers.simple_project_indexer import SimpleProjectIndexer
from backend.indexers.python_indexer import PythonIndexer
from backend.indexers.markdown_indexer import MarkdownIndexer
from backend.rag.engine import RAGEngine
import subprocess

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

async def full_reindex(project_name: str = "staffprobot"):
    """ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑÐ¼Ð¸"""
    
    logger.info("=" * 80)
    logger.info("ðŸš€ ÐŸÐžÐ›ÐÐÐ¯ ÐŸÐ•Ð Ð•Ð˜ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð˜Ð¯ Ð¡ Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð¯ÐœÐ˜")
    logger.info(f"ðŸ“¦ ÐŸÑ€Ð¾ÐµÐºÑ‚: {project_name}")
    logger.info("=" * 80)
    logger.info("")
    
    # Ð¨ÐÐ“ 1: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… QA Ð¿Ð°Ñ€
    logger.info("ðŸ“ Ð¨ÐÐ“ 1: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… QA Ð¿Ð°Ñ€ Ð¸Ð· ÐºÐ¾Ð´Ð°")
    logger.info("-" * 80)
    try:
        result = subprocess.run(
            ['python', '/app/scripts/auto_generate_qa_pairs.py', f'/projects/{project_name}'],
            capture_output=True,
            text=True,
            timeout=60
        )
        logger.info(result.stdout)
        if result.returncode != 0:
            logger.warning(f"âš ï¸ Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ QA Ð¿Ð°Ñ€ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»Ð°ÑÑŒ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹: {result.stderr}")
        else:
            logger.info("âœ… QA Ð¿Ð°Ñ€Ñ‹ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")
    except Exception as e:
        logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ QA: {e}")
    
    logger.info("")
    
    # Ð¨ÐÐ“ 2: Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‹Ñ… ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹
    logger.info("ðŸ—‘ï¸  Ð¨ÐÐ“ 2: ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‹Ñ… ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹")
    logger.info("-" * 80)
    try:
        rag_engine = RAGEngine()
        await rag_engine.initialize()
        
        collection_types = ["main", "architecture", "api", "models", "debug"]
        for ctype in collection_types:
            try:
                collection_name = f"kb_{project_name.replace('-', '_')}"
                if ctype != "main":
                    collection_name += f"_{ctype}"
                
                rag_engine.chroma_client.delete_collection(collection_name)
                logger.info(f"  âœ“ Ð£Ð´Ð°Ð»ÐµÐ½Ð° ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ: {collection_name}")
            except:
                pass  # ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð¼Ð¾Ð¶ÐµÑ‚ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ
        
        logger.info("âœ… Ð¡Ñ‚Ð°Ñ€Ñ‹Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹")
    except Exception as e:
        logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸: {e}")
    
    logger.info("")
    
    # Ð¨ÐÐ“ 3: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
    logger.info("ðŸ“š Ð¨ÐÐ“ 3: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÐºÐ¾Ð´Ð° Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸")
    logger.info("-" * 80)
    
    project_indexer = SimpleProjectIndexer()
    project_indexer.load_config()
    python_indexer = PythonIndexer()
    markdown_indexer = MarkdownIndexer()
    rag_engine = RAGEngine()
    await rag_engine.initialize()
    
    stats = {
        'total_files': 0,
        'total_chunks': 0,
        'by_collection': {}
    }
    
    async for file_info in project_indexer.iter_project_files(project_name):
        try:
            file_path = file_info['file_path']
            file_type = file_info['file_type']
            relative_path = file_info['relative_path']
            
            stats['total_files'] += 1
            
            # Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð°
            chunks = []
            if file_type == 'python':
                chunks = await python_indexer.index_file(file_path)
            elif file_type == 'markdown':
                chunks = await markdown_indexer.index_file(file_path)
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð² ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            for chunk in chunks:
                doc_type = python_indexer._classify_doc_type(relative_path) if file_type == 'python' else 'documentation'
                
                # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð² ÐºÐ°ÐºÐ¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ
                collections_to_add = ["main"]  # Ð’ÑÐµÐ³Ð´Ð° Ð² main
                
                # architecture - Ð´Ð»Ñ README, vision, Ð²Ñ‹ÑÐ¾ÐºÐ¾ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ñ‹Ñ… Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹
                if 'README' in relative_path or 'vision' in relative_path or 'doc/' in relative_path:
                    collections_to_add.append("architecture")
                
                # api - Ð´Ð»Ñ Ñ€Ð¾ÑƒÑ‚Ð¾Ð² Ð¸ API
                if doc_type in ['route', 'api', 'handler']:
                    collections_to_add.append("api")
                
                # models - Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð‘Ð”
                if doc_type in ['model', 'schema'] or 'entities' in relative_path:
                    collections_to_add.append("models")
                
                # debug - Ð´Ð»Ñ TODO/FIXME ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²
                if 'TODO' in chunk['content'] or 'FIXME' in chunk['content']:
                    collections_to_add.append("debug")
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
                for coll_type in collections_to_add:
                    collection = rag_engine.get_collection(project_name, coll_type)
                    
                    await rag_engine.store_document(
                        project=project_name,
                        content=chunk['content'],
                        metadata={
                            'file': relative_path,
                            'type': chunk['type'],
                            'doc_type': doc_type,
                            'start_line': chunk.get('start_line', 0),
                            'end_line': chunk.get('end_line', 0),
                            'lines': chunk.get('lines', '0-0'),
                            'project': project_name,
                            'collection_type': coll_type,
                            'function_name': chunk.get('function_name'),
                            'parameters': chunk.get('parameters'),
                            'return_type': chunk.get('return_type'),
                            'chunk_id': chunk.get('chunk_id', hash(chunk['content'][:100]))
                        }
                    )
                    
                    stats['by_collection'][coll_type] = stats['by_collection'].get(coll_type, 0) + 1
                
                stats['total_chunks'] += 1
            
            # ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 10 Ñ„Ð°Ð¹Ð»Ð¾Ð²
            if stats['total_files'] % 10 == 0:
                logger.info(f"  ðŸ“Š ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {stats['total_files']}, Ñ‡Ð°Ð½ÐºÐ¾Ð²: {stats['total_chunks']}")
        
        except Exception as e:
            logger.error(f"  âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ {file_info.get('relative_path')}: {e}")
    
    logger.info(f"\nâœ… Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°:")
    logger.info(f"   â€¢ Ð’ÑÐµÐ³Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {stats['total_files']}")
    logger.info(f"   â€¢ Ð’ÑÐµÐ³Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð²: {stats['total_chunks']}")
    logger.info(f"\nðŸ“š ÐŸÐ¾ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑÐ¼:")
    for coll_type, count in stats['by_collection'].items():
        logger.info(f"   â€¢ {coll_type}: {count} Ñ‡Ð°Ð½ÐºÐ¾Ð²")
    
    logger.info("")
    
    # Ð¨ÐÐ“ 4: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… QA Ð¿Ð°Ñ€
    logger.info("ðŸŽ“ Ð¨ÐÐ“ 4: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… QA Ð¿Ð°Ñ€")
    logger.info("-" * 80)
    try:
        import json
        qa_file = Path("/tmp/generated_qa_pairs.json")
        if qa_file.exists():
            with open(qa_file, 'r', encoding='utf-8') as f:
                qa_pairs = json.load(f)
            
            for pair in qa_pairs:
                training_doc = f"""
Ð’ÐžÐŸÐ ÐžÐ¡: {pair['question']}

ÐžÐ¢Ð’Ð•Ð¢: {pair['answer']}

---
ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ: {pair['metadata'].get('category', 'general')}
Ð¤Ð°Ð¹Ð»: {pair['metadata'].get('file', 'N/A')}
"""
                await rag_engine.store_document(
                    project=project_name,
                    content=training_doc,
                    metadata={
                        'file': pair['metadata'].get('file', 'training_qa'),
                        'type': 'qa_pair',
                        'doc_type': 'training',
                        'project': project_name
                    }
                )
            
            logger.info(f"âœ… Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ {len(qa_pairs)} Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… Ð¿Ð°Ñ€")
        else:
            logger.warning("âš ï¸ Ð¤Ð°Ð¹Ð» Ñ QA Ð¿Ð°Ñ€Ð°Ð¼Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼")
    except Exception as e:
        logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ QA: {e}")
    
    logger.info("")
    logger.info("=" * 80)
    logger.info("âœ… ÐŸÐ•Ð Ð•Ð˜ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð˜Ð¯ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ")
    logger.info("=" * 80)
    
    return stats

if __name__ == "__main__":
    project = sys.argv[1] if len(sys.argv) > 1 else "staffprobot"
    asyncio.run(full_reindex(project))

