#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ QA –ø–∞—Ä
–ë–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞ –≤ metadata - —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏
"""
import sys
import json
import logging
sys.path.insert(0, '/app')

from backend.rag.engine import RAGEngine

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

async def load_simplified_qa():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö QA –ø–∞—Ä"""
    logger.info("üöÄ –£–ü–†–û–©–ï–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê QA –ü–ê–†")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ QA –ø–∞—Ä—ã
    all_pairs = []
    
    files = [
        ('/tmp/targeted_qa_pairs.json', 'targeted'),
        ('/tmp/super_detailed_qa_pairs.json', 'super'),
        ('/tmp/generated_qa_pairs.json', 'generated'),
        ('/tmp/graph_qa_pairs.json', 'graph'),
        ('/tmp/massive_qa_pairs.json', 'massive'),
    ]
    
    for file_path, source in files:
        try:
            with open(file_path, 'r') as f:
                pairs = json.load(f)
                logger.info(f"‚úÖ {source}: {len(pairs)} –ø–∞—Ä")
                all_pairs.extend(pairs)
        except:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {source} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    logger.info(f"\nüìä –í–°–ï–ì–û –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_pairs)} QA –ø–∞—Ä")
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen = set()
    unique_pairs = []
    for pair in all_pairs:
        q = pair['question'].lower().strip()
        if q not in seen:
            seen.add(q)
            unique_pairs.append(pair)
    
    logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_pairs)} QA –ø–∞—Ä")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG
    engine = RAGEngine()
    await engine.initialize()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á–∞–º–∏
    batch_size = 500
    total_stored = 0
    
    for batch_idx in range(0, len(unique_pairs), batch_size):
        batch = unique_pairs[batch_idx:batch_idx + batch_size]
        
        logger.info(f"\nüíæ –ë–∞—Ç—á {batch_idx//batch_size + 1}/{(len(unique_pairs)-1)//batch_size + 1}: {len(batch)} –ø–∞—Ä")
        
        for i, pair in enumerate(batch):
            try:
                # –£–ü–†–û–©–ï–ù–ù–´–ô content (–±–µ–∑ –æ–≥—Ä–æ–º–Ω–æ–≥–æ –∫–æ–¥–∞)
                content = f"–í–æ–ø—Ä–æ—Å: {pair['question']}\n–û—Ç–≤–µ—Ç: {pair['answer'][:500]}"
                
                # –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ï metadata
                metadata = {
                    "type": "qa_pair",
                    "question": pair['question'][:200],
                    "chunk_id": f"qa_{batch_idx + i}_{abs(hash(pair['question']))}"
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–ª—è –∏–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                if "metadata" in pair:
                    for key in ['file', 'function', 'class', 'endpoint', 'line']:
                        if key in pair['metadata']:
                            val = pair['metadata'][key]
                            if isinstance(val, str):
                                metadata[key] = val[:100]
                            elif isinstance(val, int):
                                metadata[key] = str(val)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                await engine.store_document(
                    project='staffprobot',
                    content=content,
                    metadata=metadata
                )
                
                total_stored += 1
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—ã {batch_idx + i}: {str(e)[:100]}")
        
        logger.info(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(batch)} –∏–∑ –±–∞—Ç—á–∞. –í—Å–µ–≥–æ: {total_stored}")
    
    logger.info(f"\nüéâ –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_stored} QA –ø–∞—Ä")

if __name__ == "__main__":
    import asyncio
    asyncio.run(load_simplified_qa())

