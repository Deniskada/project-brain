version: '3.8'

services:
  # Ollama запускается отдельно с GPU через start_ollama_gpu.sh
  
  chromadb:
    image: chromadb/chroma:0.5.23
    container_name: project-brain-chromadb
    ports:
      - "8002:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: project-brain-redis
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  api:
    build: .
    container_name: project-brain-api
    ports:
      - "8003:8003"
    volumes:
      - .:/app
      - /home/sa/projects/staffprobot:/projects/staffprobot:ro
    environment:
      - OLLAMA_HOST=http://192.168.2.107:11434
      - CHROMA_HOST=http://chromadb:8000
      - REDIS_URL=redis://redis:6379
    command: python -m uvicorn backend.api.main:app --host 0.0.0.0 --port 8003
    deploy:
      resources:
        limits:
          cpus: 16
          memory: 16G
        reservations:
          cpus: 8
          memory: 8G
    depends_on:
      - chromadb
      - redis
    restart: unless-stopped

volumes:
  ollama_data:
  chroma_data:
  redis_data:
